{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# METEOR",
   "id": "a9ce3db66048a281"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-02T03:13:17.038821Z",
     "start_time": "2024-12-02T03:13:17.029722Z"
    }
   },
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normalize the text by lowercasing and removing punctuation.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "def get_word_matches(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Compute word matches between the reference and hypothesis.\n",
    "    \"\"\"\n",
    "    ref_words = reference.split()\n",
    "    hyp_words = hypothesis.split()\n",
    "\n",
    "    ref_counts = Counter(ref_words)\n",
    "    hyp_counts = Counter(hyp_words)\n",
    "\n",
    "    # Find intersection of words\n",
    "    matches = sum((ref_counts & hyp_counts).values())\n",
    "    return matches\n",
    "\n",
    "def compute_meteor_score(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Compute the METEOR score for a single pair of reference and hypothesis.\n",
    "    \"\"\"\n",
    "    # Normalize texts\n",
    "    reference = normalize_text(reference)\n",
    "    hypothesis = normalize_text(hypothesis)\n",
    "\n",
    "    # Calculate precision and recall\n",
    "    matches = get_word_matches(reference, hypothesis)\n",
    "    precision = matches / len(hypothesis.split()) if hypothesis.split() else 0\n",
    "    recall = matches / len(reference.split()) if reference.split() else 0\n",
    "\n",
    "    # Calculate F-measure\n",
    "    if precision + recall > 0:\n",
    "        f1_score = (10 * precision * recall) / (9 * precision + recall)\n",
    "    else:\n",
    "        f1_score = 0\n",
    "\n",
    "    # Apply penalty for word order mismatch\n",
    "    hyp_words = hypothesis.split()\n",
    "    ref_words = reference.split()\n",
    "    chunks = 0\n",
    "    i = 0\n",
    "    while i < len(hyp_words):\n",
    "        if hyp_words[i] in ref_words:\n",
    "            start_index = ref_words.index(hyp_words[i])\n",
    "            while i < len(hyp_words) and start_index < len(ref_words) and hyp_words[i] == ref_words[start_index]:\n",
    "                i += 1\n",
    "                start_index += 1\n",
    "            chunks += 1\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    penalty = 0.5 * (chunks / matches) if matches > 0 else 1\n",
    "    meteor_score = f1_score * (1 - penalty)\n",
    "\n",
    "    # Return the detailed metrics\n",
    "    return {\n",
    "        \"recall\": recall,\n",
    "        \"precision\": precision,\n",
    "        \"f1_score\": f1_score\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "reference_text = \"The quick brown fox jumps over the lazy dog\"\n",
    "hypothesis_text = \"A quick brown dog jumps over the lazy fox\"\n",
    "\n",
    "meteor_result = compute_meteor_score(reference_text, hypothesis_text)\n",
    "print(\"METEOR Result:\", meteor_result)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METEOR Result: {'recall': 0.2222222222222222, 'precision': 0.5, 'f1_score': 0.23529411764705882}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ROUGE-N\n",
    "n-gram(1-gram, 2-gram 등)을 생성하여 참조(reference)와 가설(hypothesis)의 겹침을 측정.\n",
    "Recall, Precision, F1-score를 계산:\n",
    "Recall: 겹치는 n-gram / 참조 n-gram\n",
    "Precision: 겹치는 n-gram / 가설 n-gram\n",
    "F1: Recall과 Precision의 조화 평균."
   ],
   "id": "e7da09740040aad1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T03:11:43.333143Z",
     "start_time": "2024-12-02T03:11:43.324452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "from itertools import islice\n",
    "\n",
    "def get_ngrams(text, n):\n",
    "    \"\"\"\n",
    "    Generate n-grams from text.\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    return list(zip(*[tokens[i:] for i in range(n)]))\n",
    "\n",
    "def rouge_n(reference, hypothesis, n=1):\n",
    "    \"\"\"\n",
    "    Compute ROUGE-N score.\n",
    "    \"\"\"\n",
    "    # Generate n-grams for reference and hypothesis\n",
    "    ref_ngrams = Counter(get_ngrams(reference, n))\n",
    "    hyp_ngrams = Counter(get_ngrams(hypothesis, n))\n",
    "\n",
    "    # Count overlapping n-grams\n",
    "    overlap = sum((ref_ngrams & hyp_ngrams).values())\n",
    "    total_ref_ngrams = sum(ref_ngrams.values())\n",
    "    total_hyp_ngrams = sum(hyp_ngrams.values())\n",
    "\n",
    "    # Calculate Recall, Precision, and F1-score\n",
    "    recall = overlap / total_ref_ngrams if total_ref_ngrams > 0 else 0\n",
    "    precision = overlap / total_hyp_ngrams if total_hyp_ngrams > 0 else 0\n",
    "    f1_score = (2 * recall * precision / (recall + precision)) if (recall + precision) > 0 else 0\n",
    "\n",
    "    return {\"recall\": recall, \"precision\": precision, \"f1_score\": f1_score}\n",
    "\n",
    "# Example usage\n",
    "reference = \"the cat sat on the mat\"\n",
    "hypothesis = \"the cat is on the mat\"\n",
    "\n",
    "rouge_1 = rouge_n(reference, hypothesis, n=1)\n",
    "rouge_2 = rouge_n(reference, hypothesis, n=2)\n",
    "\n",
    "print(\"ROUGE-1:\", rouge_1)\n",
    "print(\"ROUGE-2:\", rouge_2)\n"
   ],
   "id": "16d1200b5804437d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1: {'recall': 0.8333333333333334, 'precision': 0.8333333333333334, 'f1_score': 0.8333333333333334}\n",
      "ROUGE-2: {'recall': 0.6, 'precision': 0.6, 'f1_score': 0.6}\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ROUGE-L\n",
    "\n",
    "LCS(Longest Common Subsequence)를 사용해 두 텍스트 간의 겹침을 측정.\n",
    "LCS 길이를 기반으로 Recall, Precision, F1-score 계산:\n",
    "Recall: LCS 길이 / 참조 단어 수\n",
    "Precision: LCS 길이 / 가설 단어 수."
   ],
   "id": "81c243bd7e7aedd7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T03:11:58.438916Z",
     "start_time": "2024-12-02T03:11:58.430601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def lcs_length(x, y):\n",
    "    \"\"\"\n",
    "    Compute the length of the Longest Common Subsequence (LCS).\n",
    "    \"\"\"\n",
    "    m, n = len(x), len(y)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if x[i - 1] == y[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1] + 1\n",
    "            else:\n",
    "                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n",
    "\n",
    "    return dp[m][n]\n",
    "\n",
    "def rouge_l(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Compute ROUGE-L score.\n",
    "    \"\"\"\n",
    "    ref_tokens = reference.split()\n",
    "    hyp_tokens = hypothesis.split()\n",
    "\n",
    "    # Length of LCS\n",
    "    lcs = lcs_length(ref_tokens, hyp_tokens)\n",
    "\n",
    "    # Calculate Recall, Precision, and F1-score\n",
    "    recall = lcs / len(ref_tokens) if len(ref_tokens) > 0 else 0\n",
    "    precision = lcs / len(hyp_tokens) if len(hyp_tokens) > 0 else 0\n",
    "    f1_score = (2 * recall * precision / (recall + precision)) if (recall + precision) > 0 else 0\n",
    "\n",
    "    return {\"recall\": recall, \"precision\": precision, \"f1_score\": f1_score}\n",
    "\n",
    "# Example usage\n",
    "reference = \"the cat sat on the mat\"\n",
    "hypothesis = \"the cat is on the mat\"\n",
    "\n",
    "rouge_l_score = rouge_l(reference, hypothesis)\n",
    "\n",
    "print(\"ROUGE-L:\", rouge_l_score)\n"
   ],
   "id": "dd4b524724f77098",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-L: {'recall': 0.8333333333333334, 'precision': 0.8333333333333334, 'f1_score': 0.8333333333333334}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#  BERTScore 예제",
   "id": "7c8abadfb1bf7e7a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T03:39:33.109413Z",
     "start_time": "2024-12-02T03:39:22.227642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from bert_score import score\n",
    "\n",
    "# 참조 텍스트와 생성 텍스트\n",
    "references = [\"빠른 갈색 여우가 게으른 개를 뛰어넘었다.\"]\n",
    "hypotheses = [\"빠른 갈색 여우가 게으른 개를 뛰어 넘었다.\"]\n",
    "\n",
    "# BERTScore 계산\n",
    "P, R, F1 = score(hypotheses, references, lang=\"ko\", verbose=True)\n",
    "\n",
    "print(f\"Precision: {P.mean().item():.4f}\")\n",
    "print(f\"Recall: {R.mean().item():.4f}\")\n",
    "print(f\"F1 Score: {F1.mean().item():.4f}\")\n"
   ],
   "id": "6bd1d545d8c998c7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\dev\\SKN\\Final\\SKN03-FINAL-6Team\\TailorLink_LLM\\doc_retrieval_qa\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\USER\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 394.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.08 seconds, 12.02 sentences/sec\n",
      "Precision: 0.9588\n",
      "Recall: 0.9588\n",
      "F1 Score: 0.9588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# GPTScore (Perplexity 기반 평가)",
   "id": "d39a00307131f801"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install transformers",
   "id": "35cfab4e91257751"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T03:40:48.637577Z",
     "start_time": "2024-12-02T03:40:37.830584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, text):\n",
    "    \"\"\"\n",
    "    Perplexity 계산 함수\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    loss = outputs.loss\n",
    "    perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "# GPT-2 모델 로드\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# 텍스트 Perplexity 계산\n",
    "text = \"빠른 갈색 여우가 게으른 개를 뛰어넘었다.\"\n",
    "perplexity = calculate_perplexity(model, tokenizer, text)\n",
    "print(f\"Perplexity: {perplexity:.4f}\")\n"
   ],
   "id": "1ebf216011ec0116",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\dev\\SKN\\Final\\SKN03-FINAL-6Team\\TailorLink_LLM\\doc_retrieval_qa\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\USER\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 9.4079\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. Sentence Similarity with Cosine Similarity",
   "id": "a856988b814c5b5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# !pip install sentence-transformers\n",
   "id": "b0dbfe13a6ba4a03"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T05:06:14.698956Z",
     "start_time": "2024-12-02T05:06:02.922111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 모델 로드\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# 텍스트 정의\n",
    "reference = \"빠른 갈색 여우가 게으른 개를 뛰어넘었다.\"\n",
    "hypothesis = \"빠른 갈색 여우가 게으른 개를 뛰어 넘었다.\"\n",
    "\n",
    "# 임베딩 생성\n",
    "ref_embedding = model.encode([reference])\n",
    "hyp_embedding = model.encode([hypothesis])\n",
    "\n",
    "# 코사인 유사도 계산\n",
    "similarity = cosine_similarity(ref_embedding, hyp_embedding)\n",
    "print(f\"Cosine Similarity: {similarity[0][0]:.4f}\")\n"
   ],
   "id": "1cb050842c6957e3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\dev\\SKN\\Final\\SKN03-FINAL-6Team\\TailorLink_LLM\\doc_retrieval_qa\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\USER\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.9863\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "89006419ddda66ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# G-EVAL",
   "id": "a256bbee2eb871a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T05:41:35.993417Z",
     "start_time": "2024-12-02T05:41:16.707146Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install openai\n",
   "id": "2cebb66a730c1a38",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.55.3-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\dev\\skn\\final\\skn03-final-6team\\tailorlink_llm\\doc_retrieval_qa\\.venv\\lib\\site-packages (from openai) (4.6.2.post1)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\dev\\skn\\final\\skn03-final-6team\\tailorlink_llm\\doc_retrieval_qa\\.venv\\lib\\site-packages (from openai) (0.28.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.8.0-cp312-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.10.2-py3-none-any.whl.metadata (170 kB)\n",
      "Requirement already satisfied: sniffio in c:\\dev\\skn\\final\\skn03-final-6team\\tailorlink_llm\\doc_retrieval_qa\\.venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\dev\\skn\\final\\skn03-final-6team\\tailorlink_llm\\doc_retrieval_qa\\.venv\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\dev\\skn\\final\\skn03-final-6team\\tailorlink_llm\\doc_retrieval_qa\\.venv\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\dev\\skn\\final\\skn03-final-6team\\tailorlink_llm\\doc_retrieval_qa\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\dev\\skn\\final\\skn03-final-6team\\tailorlink_llm\\doc_retrieval_qa\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\dev\\skn\\final\\skn03-final-6team\\tailorlink_llm\\doc_retrieval_qa\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\dev\\skn\\final\\skn03-final-6team\\tailorlink_llm\\doc_retrieval_qa\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.1 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached pydantic_core-2.27.1-cp312-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: colorama in c:\\dev\\skn\\final\\skn03-final-6team\\tailorlink_llm\\doc_retrieval_qa\\.venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openai-1.55.3-py3-none-any.whl (389 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.8.0-cp312-none-win_amd64.whl (206 kB)\n",
      "Downloading pydantic-2.10.2-py3-none-any.whl (456 kB)\n",
      "Using cached pydantic_core-2.27.1-cp312-none-win_amd64.whl (2.0 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: pydantic-core, jiter, distro, annotated-types, pydantic, openai\n",
      "Successfully installed annotated-types-0.7.0 distro-1.9.0 jiter-0.8.0 openai-1.55.3 pydantic-2.10.2 pydantic-core-2.27.1\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T05:59:17.859757Z",
     "start_time": "2024-12-02T05:59:12.933923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "# OpenAI API 키 설정\n",
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"\"\n",
    "\n",
    "\n",
    "client = OpenAI()\n",
    "def g_eval(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    G-EVAL: GPT 기반 텍스트 평가\n",
    "    - reference: 기준 텍스트\n",
    "    - hypothesis: 평가할 텍스트\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert evaluator for language models. Please evaluate the following two texts:\n",
    "\n",
    "    Reference Text: \"{reference}\"\n",
    "    Hypothesis Text: \"{hypothesis}\"\n",
    "\n",
    "    Provide a similarity score between 0 and 100, where:\n",
    "    - 0 means the texts are completely different.\n",
    "    - 100 means the texts are identical in meaning and language quality.\n",
    "\n",
    "    Please briefly explain in Korean the reasoning behind your score.\n",
    "    \"\"\"\n",
    "    # 새로운 Chat API 호출\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # 모델 선택 (gpt-4 또는 gpt-3.5-turbo)\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # GPT의 응답에서 메시지 내용 추출\n",
    "    gpt_response = completion.choices[0].message.content\n",
    "    return gpt_response\n",
    "\n",
    "# 테스트 데이터\n",
    "reference_text = \"빠른 갈색 여우가 게으른 개를 뛰어넘었다.\"\n",
    "hypothesis_text = \"게으른 늙은 여우가 게으른 개를 뛰어 넘었다.\"\n",
    "\n",
    "# G-EVAL 결과\n",
    "result = g_eval(reference_text, hypothesis_text)\n",
    "print(\"G-EVAL 결과:\")\n",
    "print(result)\n",
    "\n"
   ],
   "id": "c97688fa66909c4b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G-EVAL 결과:\n",
      "Similarity Score: 30\n",
      "\n",
      "두 텍스트 간의 유사성은 낮습니다. 두 문장 모두 비슷한 구조를 가지고 있지만, 내용에 중요한 차이가 있습니다. \n",
      "\n",
      "1. **주어의 차이**: Reference Text에서는 \"빠른 갈색 여우\"라는 주어가 사용되었고, Hypothesis Text에서는 \"게으른 늙은 여우\"라는 다른 주어가 사용되었습니다. 이로 인해 주어의 성격이 완전히 달라집니다.\n",
      "2. **형용사의 차이**: 두 텍스트에서 여우에 대한 형용사가 다르며, 이는 문장의 의미에 영향을 미칩니다. \"빠른\"과 \"게으른\"은 상반되는 의미를 전달합니다.\n",
      "3. ** 행동은 유사하지만**: 두 문장 모두 \"뛰어넘었다\"는 행동을 포함하지만, 주어의 차이로 인해 상황의 맥락이 다릅니다.\n",
      "\n",
      "따라서 두 문장은 비슷한 구조를 가지지만, 의미적으로는 상당한 차이가 있으므로 30점으로 평가했습니다.\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "743dc07fa14a6330"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
